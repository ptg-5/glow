# llm.py

import sys
from hailo_platform.genai import LLM
from hailo_platform import VDevice
from typing import List, Dict

class LLMResponseGenerator:
    """
    LLM ì‘ë‹µ ìƒì„± í´ë˜ìŠ¤ - ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µí•˜ê³  ìµœì¢… í…ìŠ¤íŠ¸ ë°˜í™˜
    """
    def __init__(self, vdevice: VDevice, llm_hef: str, lora_name: str = ""):
        self.vdevice = vdevice
        self.llm_hef = llm_hef
        self.lora_name = lora_name
        self.llm_model = None

    def __enter__(self):
        """LLM ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ§  LLM: ëª¨ë¸ ë¡œë“œ ì¤‘...")
        try:
            self.llm_model = LLM(self.vdevice, self.llm_hef, self.lora_name)
            print("âœ… LLM: ì¤€ë¹„ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}", file=sys.stderr)
            raise
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """LLM ëª¨ë¸ í•´ì œ"""
        self._safe_release(self.llm_model)
        self.llm_model = None

    def _safe_release(self, obj):
        if obj is None:
            return
        try:
            if hasattr(obj, "release"):
                obj.release()
            elif hasattr(obj, "__exit__"):
                obj.__exit__(None, None, None)
        except:
            pass

    def generate_response(self, user_prompt: str, 
                         max_tokens: int = 150,
                         system_prompt: str = "You are a helpful AI assistant.") -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ LLM ì‘ë‹µ ìƒì„±
        
        Args:
            user_prompt: STTë¡œ ì¸ì‹ëœ ì‚¬ìš©ì ì§ˆë¬¸
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: LLMì˜ ìµœì¢… ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        if not self.llm_model:
            raise RuntimeError("LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        if not user_prompt.strip():
            print("âš ï¸ ë¹ˆ í”„ë¡¬í”„íŠ¸ - LLM í˜¸ì¶œ ê±´ë„ˆëœ€")
            return ""

        structured_prompt: List[Dict[str, str]] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        print("\nğŸ’¬ LLM ì‘ë‹µ ìƒì„± ì¤‘...\n")
        print("â”€" * 60)
        
        full_response = ""
        
        try:
            with self.llm_model.generate(structured_prompt, 
                                        max_generated_tokens=max_tokens, 
                                        seed=31) as gen:
                for token in gen:
                    print(token, end="", flush=True)
                    full_response += token
                    
            print("\n" + "â”€" * 60)
            print("âœ… ì‘ë‹µ ì™„ë£Œ\n")
            
        except Exception as e:
            print(f"\nâŒ LLM ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}", file=sys.stderr)
        
        return full_response.strip()